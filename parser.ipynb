{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/chat'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/guanaco-13B-GPTQ\")\n",
    "history = {'internal': [], 'visible': []}\n",
    "command = \"You are an API that converts bodies of text into a single question and answer into a JSON format. Each JSON \" \\\n",
    "          \"contains a single question with a single answer. Only respond with the JSON and no additional text. \\n\"\n",
    "\n",
    "def run(user_input, history):\n",
    "    request = {\n",
    "        'user_input': user_input,\n",
    "        'history': history,\n",
    "        'mode': 'instruct',  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "        'character': 'Example',\n",
    "        'instruction_template': 'Vicuna-v1.1',\n",
    "        'your_name': 'You',\n",
    "        'regenerate': False,\n",
    "        '_continue': False,\n",
    "        'stop_at_newline': False,\n",
    "        'chat_prompt_size': 2048,\n",
    "        'chat_generation_attempts': 1,\n",
    "        'chat-instruct_command': '',\n",
    "        'max_new_tokens': 500,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    result = response.json()['results'][0]['history']\n",
    "    return result['visible'][-1][1]\n",
    "\n",
    "def read_txt(file_path):\n",
    "    file_obj = open(file_path)\n",
    "    text = file_obj.read()\n",
    "    file_obj.close()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    enc = tokenizer.encode(text)\n",
    "    return enc\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def is_json(data):\n",
    "    try:\n",
    "        json.loads(data)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def submit_to_api(chunk, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = run(command + chunk.strip(), history)\n",
    "            # Extract JSON string from between back-ticks\n",
    "            if is_json(response):\n",
    "                print(response)\n",
    "                return json.loads(response)\n",
    "            else:\n",
    "                match = re.search(r'`(.*?)`', response, re.S)\n",
    "                if match and is_json(match.group(1)):\n",
    "                    print(f\"Attempt {i + 1} failed. Retrying...\")\n",
    "                    return json.loads(match.group(1))  # assuming you want to return the JSON data\n",
    "                else:\n",
    "                    print(f\"Request failed: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            continue\n",
    "    print(\"Max retries exceeded. Skipping this chunk.\")\n",
    "    return None\n",
    "\n",
    "text = extract_text_from_pdf('all_content.txt')\n",
    "tokens = tokenize(text)\n",
    "\n",
    "token_chunks = list(chunks(tokens, 256))\n",
    "\n",
    "responses = []\n",
    "\n",
    "for chunk in token_chunks:\n",
    "    response = submit_to_api(tokenizer.decode(chunk))\n",
    "    if response is not None:\n",
    "        responses.append(response)\n",
    "\n",
    "# Write responses to a JSON file\n",
    "with open('responses.json', 'w') as f:\n",
    "    json.dump(responses, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
